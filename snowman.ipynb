{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 1\n",
    "# first import the neccessary packages and modules\n",
    "from utils import *\n",
    "from logic import *\n",
    "from notebook import psource\n",
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "from skfuzzy import control as ctrl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import discord\n",
    "import os\n",
    "import nest_asyncio\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "nest_asyncio.apply()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 2\n",
    "# read in data from .csv file\n",
    "song_data = pd.read_csv('ChristmasSongs.csv')\n",
    "\n",
    "# check if any cells from any rows are empty\n",
    "song_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 3\n",
    "# delete the rows with empty 'Description' cells\n",
    "song_data.dropna(subset=['Description'], inplace=True)\n",
    "\n",
    "# reset the dataframe's index\n",
    "song_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "song_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 4\n",
    "# check min and max year\n",
    "min_year = song_data['Year'].min()\n",
    "max_year = song_data['Year'].max()\n",
    "\n",
    "print(f\"Minimum Year: {min_year}\")\n",
    "print(f\"Maximum Year: {max_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 5\n",
    "# create a function to categorize years into classes\n",
    "def classify_year(year):\n",
    "    if 1920 <= year <= 1950:\n",
    "        return 'Vintage Songs'\n",
    "    elif 1951 <= year <= 1980:\n",
    "        return 'Classic Songs'\n",
    "    elif 1981 <= year <= 2023:\n",
    "        return 'Modern Songs'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    \n",
    "# apply the function to create the 'Era' column\n",
    "song_data['Era'] = song_data['Year'].apply(classify_year)\n",
    "\n",
    "# display the updated dataset\n",
    "song_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 6\n",
    "# one hot encoding for the input column\n",
    "year_dummies = pd.get_dummies(song_data['Year'], prefix='Year', drop_first=True)\n",
    "\n",
    "# drop the existing columns of Year\n",
    "song_data = song_data.drop('Year', axis=1)\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "song_data = pd.concat([song_data, year_dummies], axis=1)\n",
    "\n",
    "song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 7\n",
    "# to get all the columns that starts with each prefix via regex due to one hot encoding\n",
    "year_columns = song_data.filter(regex='^Year').columns\n",
    "\n",
    "# selecting data from columns\n",
    "x_year = song_data[year_columns].values\n",
    "\n",
    "# setting the x (input) and y (output) respectively\n",
    "x = x_year\n",
    "y = song_data['Era'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 8\n",
    "# Train-Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (80% train, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 9\n",
    "# training the model with Naive Bayes model (Classification)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 10\n",
    "# training the model with Logistic Regression model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 11\n",
    "# training the model with Decision Tree model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion='gini',\n",
    "                                   random_state=100,\n",
    "                                   max_depth=3, \n",
    "                                   min_samples_leaf=3)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 12\n",
    "# evaluating the model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 13\n",
    "import pickle\n",
    "\n",
    "# save the iris classification model as a pickle file\n",
    "model_pkl_file = \"song_classifier_model.pkl\"  \n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n",
    "\n",
    "year_columns_pkl_file = \"year_columns.pkl\"  \n",
    "\n",
    "with open(year_columns_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(year_columns, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 14\n",
    "# load model from pickle file\n",
    "model_pkl_file = \"song_classifier_model.pkl\"  \n",
    "with open(model_pkl_file, 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "\n",
    "year_columns_pkl_file = \"year_columns.pkl\"  \n",
    "with open(year_columns_pkl_file, 'rb') as file:  \n",
    "    year_columns = pickle.load(file)\n",
    "\n",
    "# Create a DataFrame with a single row containing the year to predict\n",
    "data_to_predict = pd.DataFrame({\"Year\": [\"2000\"]})\n",
    "\n",
    "# Apply one-hot encoding\n",
    "data_encoded = pd.get_dummies(data_to_predict)\n",
    "\n",
    "# Reindex to match the columns used during training\n",
    "data_encoded = data_encoded.reindex(columns=year_columns, fill_value=0)\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(data_encoded)\n",
    "\n",
    "# check results\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 15\n",
    "# read in data from .csv file\n",
    "category_data = pd.read_csv('Category.csv')\n",
    "\n",
    "# check if any cells from any rows are empty\n",
    "category_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 16\n",
    "# apply NLP algorithm for 'Description' column\n",
    "corpus = []\n",
    "for i in range(0, 310):\n",
    "    #remove non-alphabet, any non-alphabet, []\n",
    "    description = re.sub('[^a-zA-Z]', ' ', category_data['Questions'][i])\n",
    "    #all text become lowercase\n",
    "    description = description.lower()\n",
    "    #split the sentence to each word (token)\n",
    "    description = description.split()\n",
    "    description = ' '.join(description)\n",
    "    corpus.append(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 17\n",
    "# create Bag of Words (BoW) model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "cv.fit(corpus)\n",
    "# setting the x (input) and y (output) respectively\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "y = category_data['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 18\n",
    "# Train-Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (80% train, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 19\n",
    "# training the model with Naive Bayes model (Classification)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 20\n",
    "# training the model with Logistic Regression model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 21\n",
    "# training the model with Decision Tree model \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion='gini',\n",
    "                                   random_state=100,\n",
    "                                   max_depth=3, \n",
    "                                   min_samples_leaf=3)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 22\n",
    "# evaluating the model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 23\n",
    "# save the iris classification model as a pickle file\n",
    "model_pkl_file = \"category_classifier.pkl\"  \n",
    "\n",
    "with open(model_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n",
    "\n",
    "vectorizer_pkl_file = \"vectorizer.pkl\"  \n",
    "\n",
    "with open(vectorizer_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(cv, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 24\n",
    "# load model from pickle file\n",
    "model_pkl_file = \"category_classifier.pkl\"  \n",
    "with open(model_pkl_file, 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "\n",
    "vectorizer_pkl_file = \"vectorizer.pkl\"  \n",
    "with open(vectorizer_pkl_file, 'rb') as file:  \n",
    "    cv = pickle.load(file)\n",
    "\n",
    "# Create a DataFrame with a single row containing the year to predict\n",
    "data_to_predict = pd.DataFrame({'Question': [\"What is the era of a song from 1970 classified as?\"]})\n",
    "\n",
    "# apply NLP algorithm for processing user input\n",
    "#remove non-alphabet, any non-alphabet, []\n",
    "corpus2 = []\n",
    "cleaned_data = re.sub('[^a-zA-Z]', ' ', data_to_predict['Question'][0])\n",
    "#all text become lowercase\n",
    "cleaned_data = cleaned_data.lower()\n",
    "#split the sentence to each word (token)\n",
    "cleaned_data = cleaned_data.split()\n",
    "cleaned_data = ' '.join(cleaned_data)\n",
    "corpus2.append(cleaned_data)\n",
    "\n",
    "test_data = cv.transform(corpus2).toarray()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# check results\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 25\n",
    "# apply fuzzy logic to calculate the temperature of weather\n",
    "\n",
    "# set the antecedent\n",
    "temp = ctrl.Antecedent(np.arange(0, 111, 1), 'temperature')\n",
    "\n",
    "# set the consequent\n",
    "category = ctrl.Consequent(np.arange(0, 111, 1), 'weather_category')\n",
    "\n",
    "temp['low'] = fuzz.trapmf(temp.universe, [0, 0, 20, 40])\n",
    "temp['moderate'] = fuzz.trimf(temp.universe, [20, 50, 80])\n",
    "temp['high'] = fuzz.trapmf(temp.universe, [60, 80, 100, 100])\n",
    "\n",
    "category['cold'] = fuzz.trapmf(temp.universe, [0, 0, 20, 40])\n",
    "category['warm'] = fuzz.trimf(temp.universe, [20, 50, 80])\n",
    "category['hot'] = fuzz.trapmf(temp.universe, [60, 80, 100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 26\n",
    "# apply simple rule sets\n",
    "rule1 = ctrl.Rule(temp['low'], category['cold'])\n",
    "rule2 = ctrl.Rule(temp['moderate'], category['warm'])\n",
    "rule3 = ctrl.Rule(temp['high'], category['hot'])\n",
    "\n",
    "# create control system\n",
    "weather_ctrl = ctrl.ControlSystem([rule1, rule2, rule3])\n",
    "weather_system = ctrl.ControlSystemSimulation(weather_ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 27\n",
    "weather_system_pkl_file = \"weather_system.pkl\"  \n",
    "\n",
    "with open(weather_system_pkl_file, 'wb') as file:  \n",
    "    pickle.dump(weather_system, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 28\n",
    "# all functions\n",
    "\n",
    "# api call to return current weather status\n",
    "def get_temperature():\n",
    "    api_key = \"YOUR_API_KEY\"\n",
    "    weatherMapUrl = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "    city = \"penang,mys\"\n",
    "    units = \"metric\"\n",
    "\n",
    "    url = weatherMapUrl + \"q=\" + city + \"&\" + \"appid=\" + api_key + \"&\" + \"units=\" + units\n",
    "\n",
    "    response = requests.get(url).json()\n",
    "\n",
    "    temperature = response['main']['temp']\n",
    "    return temperature\n",
    "\n",
    "def predict_weather():\n",
    "    weather_system_pkl_file = \"weather_system.pkl\"\n",
    "    with open(weather_system_pkl_file, 'rb') as file:  \n",
    "        weather_system = pickle.load(file)\n",
    "\n",
    "    temperature = get_temperature()\n",
    "\n",
    "    weather_system.input['temperature'] = temperature\n",
    "\n",
    "    weather_system.compute()\n",
    "\n",
    "    # get the category membership degree\n",
    "    weather_category_degree = weather_system.output['weather_category']\n",
    "\n",
    "    # determine the category based on the degree\n",
    "    if weather_category_degree >= 0 and weather_category_degree < 33.33:\n",
    "        weather_message = \"It feels cold. \" + \"The temperature is \" + str(temperature)\n",
    "    elif weather_category_degree >= 33.33 and weather_category_degree < 66.66:\n",
    "        weather_message = \"It feels warm. \" + \"The temperature is \" + str(temperature)\n",
    "    else:\n",
    "        weather_message = \"It feels hot. \" + \"The temperature is \" + str(temperature)\n",
    "\n",
    "    return weather_message\n",
    "\n",
    "def predict_category(userInput):\n",
    "    # load model from pickle file\n",
    "    model_pkl_file = \"category_classifier.pkl\"  \n",
    "    with open(model_pkl_file, 'rb') as file:  \n",
    "        model = pickle.load(file)\n",
    "\n",
    "    vectorizer_pkl_file = \"vectorizer.pkl\"  \n",
    "    with open(vectorizer_pkl_file, 'rb') as file:  \n",
    "        cv = pickle.load(file)\n",
    "\n",
    "    # Create a DataFrame with a single row containing the year to predict\n",
    "    data_to_predict = pd.DataFrame({'Question': [userInput]})\n",
    "\n",
    "    # apply NLP algorithm for processing user input\n",
    "    #remove non-alphabet, any non-alphabet, []\n",
    "    corpus2 = []\n",
    "    cleaned_data = re.sub('[^a-zA-Z]', ' ', data_to_predict['Question'][0])\n",
    "    #all text become lowercase\n",
    "    cleaned_data = cleaned_data.lower()\n",
    "    #split the sentence to each word (token)\n",
    "    cleaned_data = cleaned_data.split()\n",
    "    cleaned_data = ' '.join(cleaned_data)\n",
    "    corpus2.append(cleaned_data)\n",
    "\n",
    "    test_data = cv.transform(corpus2).toarray()\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(test_data)\n",
    "\n",
    "    # check results\n",
    "    return predictions\n",
    "\n",
    "def predict_song_era(userInput):\n",
    "    # load model from pickle file\n",
    "    model_pkl_file = \"song_classifier_model.pkl\"  \n",
    "    with open(model_pkl_file, 'rb') as file:  \n",
    "        model = pickle.load(file)\n",
    "\n",
    "    year_columns_pkl_file = \"year_columns.pkl\"  \n",
    "    with open(year_columns_pkl_file, 'rb') as file:  \n",
    "        year_columns = pickle.load(file)\n",
    "\n",
    "    yearInput = re.findall(r'\\d+', userInput)\n",
    "\n",
    "    # user asking for era prediction\n",
    "    # Create a DataFrame with a single row containing the year to predict\n",
    "    data_to_predict = pd.DataFrame({\"Year\": [yearInput[0]]})\n",
    "\n",
    "    # Apply one-hot encoding\n",
    "    data_encoded = pd.get_dummies(data_to_predict)\n",
    "\n",
    "    # Reindex to match the columns used during training\n",
    "    data_encoded = data_encoded.reindex(columns=year_columns, fill_value=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(data_encoded)\n",
    "\n",
    "    # check results\n",
    "    return predictions[0]\n",
    "\n",
    "def get_song():\n",
    "    song_data = pd.read_csv('ChristmasSongs.csv')\n",
    "    # delete the rows with empty 'Description' cells\n",
    "    song_data.dropna(subset=['Description'], inplace=True)\n",
    "\n",
    "    # reset the dataframe's index\n",
    "    song_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    random_row = song_data.sample(n=1)\n",
    "\n",
    "    # extract value from the datarow\n",
    "    title = random_row['Title'].values[0]\n",
    "    artist = random_row['Artist'].values[0]\n",
    "    year = random_row['Year'].values[0]\n",
    "\n",
    "    recommendation = \"You should listen to \" + title + \" by \" + artist + \" that is released in the year \" + str(year)\n",
    "\n",
    "    return recommendation\n",
    "\n",
    "def get_greetings():\n",
    "    response = requests.get(\"https://www.greetingsapi.com/random\").json()\n",
    "    message = response['greeting'] + \"! \" + \"That is \" + response['type'] + \" in \" + response['language']\n",
    "    \n",
    "    return message\n",
    "\n",
    "def get_event():\n",
    "    event_data = pd.read_csv('ChristmasEvents.csv')\n",
    "\n",
    "    random_row = event_data.sample(n=1)\n",
    "\n",
    "    title = random_row['EventTitle'].values[0]\n",
    "    month = random_row['Month'].values[0]\n",
    "    date = random_row['Date'].values[0]\n",
    "    location = random_row['Location'].values[0]\n",
    "\n",
    "    recommendation = \"You should go to \" + location + \" as there will be \" + title + \" on \" + str(date) + \" \" + month\n",
    "\n",
    "    return recommendation\n",
    "\n",
    "def tfidf_vectorize(text_list):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    return vectorizer.fit_transform(text_list)\n",
    "\n",
    "def get_similarity(user_desc, data_desc):\n",
    "    # Vectorize the two descriptions\n",
    "    tfidf_matrix = tfidf_vectorize([user_desc, data_desc])\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[0:1], tfidf_matrix).flatten()\n",
    "    return cosine_similarities[1]\n",
    "\n",
    "def get_history(userInput):\n",
    "    history_data = pd.read_csv('ChristmasHistory.csv')\n",
    "\n",
    "    history_data['similarity'] = history_data['Description'].apply(lambda x: get_similarity(userInput, x))\n",
    "    prediction = history_data.sort_values(by='similarity', ascending=False).head(2)\n",
    "\n",
    "    cleaned_prediction = prediction['Description'].tolist()\n",
    "    cleaned_prediction = \",\".join(cleaned_prediction)\n",
    "    cleaned_prediction = re.sub(r'\\[\\d+\\]', '', cleaned_prediction)\n",
    "\n",
    "    return cleaned_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 29\n",
    "# initialize the bot\n",
    "token = \"YOUR_DISCORD_BOT_TOKEN\"\n",
    "\n",
    "client = discord.Client(intents=discord.Intents.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 30\n",
    "class MyClient(discord.Client):\n",
    "    async def on_ready(self):\n",
    "        print(f'Logged in as {self.user} (ID: {self.user.id})')\n",
    "        print('------')\n",
    "        print('Ready!')\n",
    "\n",
    "    async def on_message(self, message):\n",
    "        # we do not want the bot to reply to itself\n",
    "        if message.author.id == self.user.id:\n",
    "            return\n",
    "        \n",
    "        category = predict_category(message.content)\n",
    "\n",
    "        # while the bot is waiting on a response from the model\n",
    "        # set the its status as typing for user-friendliness\n",
    "        async with message.channel.typing():\n",
    "            if (category[0]) == \"Greetings\":\n",
    "                reply = get_greetings()\n",
    "                await message.channel.send(reply)\n",
    "            elif (category[0] == \"Weather\"):\n",
    "                weather_result = str(predict_weather())\n",
    "                await message.channel.send(weather_result)\n",
    "            elif (category[0] == \"SongsEra\"):\n",
    "                prediction = predict_song_era(message.content)\n",
    "                await message.channel.send(\"The era of the song is classified as \" + prediction)\n",
    "            elif (category[0] == \"Songs\"):\n",
    "                recommendation = get_song()\n",
    "                await message.channel.send(recommendation)\n",
    "            elif (category[0] == \"Events\"):\n",
    "                prediction = get_event()\n",
    "                await message.channel.send(prediction)\n",
    "            elif (category[0] == \"History\"):\n",
    "                prediction = get_history(message.content)\n",
    "                await message.channel.send(prediction)\n",
    "            # any exceptions\n",
    "            else:\n",
    "                await message.channel.send(\"I am sorry. I did not get that. Can you please rephrase?\")\n",
    "\n",
    "intents = discord.Intents.default()\n",
    "intents.message_content = True\n",
    "\n",
    "def main():\n",
    "  client = MyClient(intents=intents)\n",
    "  client.run(token)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
